---
title: "Building Dialogue Models for Affective Prosthetics"
date: "2025-01-08"
summary: "Adapting my affective computing research into an explainable companion that lives inside a smart prosthetic."
tags: ["research", "ai", "prosthetics"]
image: "/images/blog/affective-dialogue.svg"
---

My dissertation work looks at how conversational agents can support people using assistive
prosthetics. Translating those findings into a production system meant bridging qualitative research
with latency budgets, hardware constraints, and privacy expectations.

## Modeling with constraints

I fine-tuned a compact transformer on our longitudinal interview transcripts. To keep the model small
enough for embedded deployment, I distilled it with a curriculum that emphasized empathy markers and
supportive follow-ups over wide-ranging small talk.

The policy layer runs in a Rust microservice that enforces deterministic turn-taking. A short excerpt:

```rust
fn schedule_prompt(state: &DialogState) -> PromptPlan {
    if state.last_affect > AFFECT_THRESHOLD {
        return PromptPlan::acknowledge("I can slow down or reschedule if today is a lot.");
    }

    if state.pending_tasks.is_empty() {
        return PromptPlan::suggest_activity("Want a two-minute mobility stretch?");
    }

    PromptPlan::check_in("How is your socket feeling right now?")
}
```

## Privacy-in by design

No audio leaves the device; transcripts are encrypted with the participant's key and synced over a
Tor hidden service for researchers. That architecture earned quick buy-in from the IRB because we can
demonstrate consent-aware data handling end-to-end.

The production deployment is rolling out to five testers this spring. I will keep sharing what works
(and what doesn't) as we iterate alongside our participants.
